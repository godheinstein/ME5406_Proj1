{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from frozenlake_env import FrozenLakeEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for reproducibility\n",
    "np.random.seed(25)\n",
    "random.seed(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "GAMMA = 0.99  # discount factor\n",
    "EPSILON = 0.1  # exploration rate\n",
    "EPISODES = 2000  # no. of episodes\n",
    "EPSILON_DECAY = 0.995  # epsilon decay rate\n",
    "EPSILON_MIN = 0.01  # minimum epsilon\n",
    "GRID_SIZE = 4  # grid size (4x4 or 10x10)\n",
    "HOLE_FRACTION = 0.25  # fraction of holes\n",
    "USE_DEFAULT_MAP = False  # use default map if available\n",
    "\n",
    "# create environment\n",
    "env = FrozenLakeEnv(grid_size=GRID_SIZE, hole_fraction=HOLE_FRACTION, use_default_map=USE_DEFAULT_MAP)\n",
    "\n",
    "class MonteCarloControl: \n",
    "    def __init__(self, env, gamma=GAMMA, epsilon=EPSILON, epsilon_decay=EPSILON_DECAY, epsilon_min=EPSILON_MIN):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.Q = defaultdict(lambda: np.zeros(len(env.actions)))\n",
    "        self.returns = defaultdict(list)\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(list(self.env.actions.keys()))  # Explore\n",
    "        else:\n",
    "            return max(self.env.actions.keys(), key=lambda a: self.Q[state][a])  # Exploit\n",
    "\n",
    "    def train(self, episodes):\n",
    "        for episode in range(episodes):\n",
    "            state = env.reset()\n",
    "            episode_history = []\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            # generate episode\n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                episode_history.append((state, action, reward))\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "\n",
    "            # update Q-values using first-visit MC\n",
    "            G = 0\n",
    "            for t in range(len(episode_history) - 1, -1, -1):\n",
    "                state, action, reward = episode_history[t]\n",
    "                G = self.gamma * G + reward\n",
    "                if (state, action) not in [(x[0], x[1]) for x in episode_history[:t]]:\n",
    "                    self.returns[(state, action)].append(G)\n",
    "                    self.Q[state][action] = np.mean(self.returns[(state, action)])\n",
    "\n",
    "            # decay epsilon\n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)  \n",
    "            # record episode reward\n",
    "            self.episode_rewards.append(total_reward)\n",
    "        return self.Q, self.episode_rewards\n",
    "    \n",
    "    def visualize_policy(self, Q, env):\n",
    "        policy = np.zeros((env.grid_size, env.grid_size), dtype=str)\n",
    "        for state in Q:\n",
    "            row, col = state // env.grid_size, state % env.grid_size\n",
    "            action = np.argmax(Q[state])\n",
    "            policy[row, col] = env.actions[action][0]  # Use first letter of action (U, D, R, L)\n",
    "        print(\"Learned Policy:\")\n",
    "        print(policy)\n",
    "\n",
    "\n",
    "    def test_policy(Q, env, episodes=10):\n",
    "        total_rewards = 0\n",
    "        for episode in range(episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                action = np.argmax(Q[state])  # Greedy policy\n",
    "                next_state, reward, done = env.step(action)\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "            total_rewards += episode_reward\n",
    "            print(f\"Episode {episode + 1}: Reward = {episode_reward}\")\n",
    "        print(f\"Average Reward over {episodes} episodes: {total_rewards / episodes}\")\n",
    "\n",
    "    \n",
    "\n",
    "    def visualize_value_function(Q, env):\n",
    "        value_function = np.zeros((env.grid_size, env.grid_size))\n",
    "        for state in Q:\n",
    "            row, col = state // env.grid_size, state % env.grid_size\n",
    "            value_function[row, col] = np.max(Q[state])\n",
    "        print(\"Value Function:\")\n",
    "        print(value_function)\n",
    "\n",
    "    def render_policy(Q, env):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = np.argmax(Q[state])  # Greedy policy\n",
    "            next_state, reward, done = env.step(action)\n",
    "            state = next_state\n",
    "        env.final()  # Show the final path\n",
    "\n",
    "\n",
    "    def save_q_table_text(self, filename):\n",
    "        \"\"\"\n",
    "        Save the Q-table to a plain text file.\n",
    "        \"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            for state, actions in self.Q.items():\n",
    "                action_values = ' '.join(map(str, actions))\n",
    "                f.write(f\"{state} {action_values}\\n\")\n",
    "\n",
    "    def load_q_table_text(self, filename):\n",
    "        \"\"\"\n",
    "        Load the Q-table from a plain text file.\n",
    "        \"\"\"\n",
    "        self.Q = defaultdict(lambda: np.zeros(len(self.env.actions)))\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                state = int(parts[0])\n",
    "                actions = np.array([float(x) for x in parts[1:]])\n",
    "                self.Q[state] = actions   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training with Monte Carlo Control...\")\n",
    "mc_agent = MonteCarloControl(env)\n",
    "Q_mc, mc_rewards = mc_agent.train(EPISODES)\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# save the Q-table\n",
    "mc_agent.save_q_table_text('q_table_mc.txt')\n",
    "print(\"Q-table saved to 'q_table_mc.txt'\")\n",
    "\n",
    "# load the Q-table (optional, if want to test without retraining)\n",
    "mc_agent.load_q_table_text('q_table_mc.txt')\n",
    "print(\"Q-table loaded from 'q_table_mc.txt'\")\n",
    "\n",
    "# visualize the policy \n",
    "mc_agent.visualize_policy(Q_mc, env)  \n",
    "# test the policy\n",
    "mc_agent.test_policy(Q_mc, env)\n",
    "# visualize the value function\n",
    "mc_agent.visualize_value_function(Q_mc, env)\n",
    "# render the policy\n",
    "mc_agent.render_policy(Q_mc, env)\n",
    "\n",
    "# plot cumulative rewards\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.cumsum(mc_rewards), label=f\"Monte Carlo Control (Map Size: {GRID_SIZE}x{GRID_SIZE})\")\n",
    "plt.title(\"Cumulative Rewards Over Episodes\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot moving average of rewards\n",
    "window_size = 100  # Moving average window\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.convolve(mc_rewards, np.ones(window_size)/window_size, mode='valid'),\n",
    "         label=f\"Monte Carlo Control (Map Size: {GRID_SIZE}x{GRID_SIZE})\")\n",
    "plt.title(f\"Moving Average of Rewards (Window Size = {window_size})\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ME5406_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
