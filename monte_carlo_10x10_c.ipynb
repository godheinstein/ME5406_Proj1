{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from frozenlake_env import FrozenLakeEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for reproducibility\n",
    "np.random.seed(25)\n",
    "random.seed(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "GAMMA = 0.99  # discount factor\n",
    "#EPSILON = 0.1  # exploration rate\n",
    "EPISODES = 2000  # no. of episodes\n",
    "#EPSILON_DECAY = 0.995  # epsilon decay rate\n",
    "#EPSILON_MIN = 0.01  # minimum epsilon\n",
    "GRID_SIZE = 10  # grid size (4x4 or 10x10)\n",
    "HOLE_FRACTION = 0.25  # fraction of holes\n",
    "USE_DEFAULT_MAP = True  # use default map if available\n",
    "\n",
    "# create environment\n",
    "env = FrozenLakeEnv(grid_size=GRID_SIZE, hole_fraction=HOLE_FRACTION, use_default_map=USE_DEFAULT_MAP)\n",
    "\n",
    "\n",
    "class MonteCarloControl: \n",
    "    def __init__(self, env, gamma=GAMMA):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.Q = defaultdict(lambda: np.zeros(len(env.actions)))\n",
    "        self.returns = defaultdict(list)\n",
    "        self.episode_rewards = []\n",
    "        self.record_goal = []\n",
    "        self.record_fail = []\n",
    "        self.record_path_length = []\n",
    "\n",
    "    # no epsilon greedy behaviour policy, choose the greedy action (no exploration)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        return max(self.env.actions.keys(), key=lambda a: self.Q[state][a])  # Exploit\n",
    "\n",
    "    def train(self, episodes):\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            episode_history = []\n",
    "            episode_rewards = 0\n",
    "            done = False\n",
    "            path_length = 0\n",
    "\n",
    "            # generate episode\n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                episode_history.append((state, action, reward))\n",
    "                episode_rewards += reward\n",
    "                state = next_state\n",
    "                path_length += 1\n",
    "\n",
    "            self.episode_rewards.append(episode_rewards)\n",
    "            self.record_path_length.append(path_length)\n",
    "\n",
    "            if reward == 1:\n",
    "                self.record_goal.append(1)\n",
    "                self.record_fail.append(0)\n",
    "            else:\n",
    "                self.record_goal.append(0)\n",
    "                self.record_fail.append(1)\n",
    "\n",
    "            # update Q-values using first-visit MC\n",
    "            G = 0\n",
    "            visited_state_actions = set()\n",
    "            for t in range(len(episode_history) - 1, -1, -1):\n",
    "                state, action, reward = episode_history[t]\n",
    "                G = self.gamma * G + reward\n",
    "                if (state, action) not in visited_state_actions:\n",
    "                    visited_state_actions.add((state, action))\n",
    "                    self.returns[(state, action)].append(G)\n",
    "                    self.Q[state][action] = np.mean(self.returns[(state, action)])\n",
    "\n",
    "        return self.Q, self.episode_rewards\n",
    "    \n",
    "    # Save the Q-table to a plain text file.\n",
    "    def save_q_table_text(self, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            for state, actions in self.Q.items():\n",
    "                action_values = ' '.join(map(str, actions))\n",
    "                f.write(f\"{state} {action_values}\\n\")\n",
    "                \n",
    "    # Load the Q-table from a plain text file.\n",
    "    def load_q_table_text(self, filename):\n",
    "        self.Q = defaultdict(lambda: np.zeros(len(env.actions)))\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                state = int(parts[0])\n",
    "                actions = np.array([float(x) for x in parts[1:]])\n",
    "                self.Q[state] = actions   \n",
    "    \n",
    "    def visualize_policy(self, Q, env):\n",
    "        policy = np.zeros((env.grid_size, env.grid_size), dtype=str)\n",
    "        for state in Q:\n",
    "            row, col = state // env.grid_size, state % env.grid_size\n",
    "            action = np.argmax(Q[state])\n",
    "            policy[row, col] = env.actions[action][0]  # Use first letter of action (U, D, R, L)\n",
    "        print(\"Learned Policy:\")\n",
    "        print(policy)\n",
    "\n",
    "\n",
    "    def test_policy(self, Q, env, episodes=10):\n",
    "        total_rewards = 0\n",
    "        for episode in range(episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                action = np.argmax(Q[state])  \n",
    "                next_state, reward, done = env.step(action)\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "            total_rewards += episode_reward\n",
    "            print(f\"Episode {episode + 1}: Reward = {episode_reward}\")\n",
    "        print(f\"Average Reward over {episodes} episodes: {total_rewards / episodes}\")\n",
    "\n",
    "    def visualize_value_function(self, Q, env):\n",
    "        value_function = np.zeros((env.grid_size, env.grid_size))\n",
    "        for state in Q:\n",
    "            row, col = state // env.grid_size, state % env.grid_size\n",
    "            value_function[row, col] = np.max(Q[state])\n",
    "        print(\"Value Function:\")\n",
    "        print(value_function)\n",
    "\n",
    "    def render_policy(self, Q, env):\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = np.argmax(Q[state])  # Greedy policy\n",
    "            next_state, reward, done = env.step(action)\n",
    "            state = next_state\n",
    "        env.final()  # Show the final path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the performance of the robot in the figures\n",
    "def plot_results(record_goal, record_fail, record_path_length, reward_list, performance_bar):\n",
    "    fig = plt.figure()\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=0.7)\n",
    "    f1 = fig.add_subplot(2, 2, 1)\n",
    "    f2 = fig.add_subplot(2, 2, 2)\n",
    "    f3 = fig.add_subplot(2, 2, 3)\n",
    "    f4 = fig.add_subplot(2, 2, 4)\n",
    "\n",
    "    # Plot the No. of Successful episodes\n",
    "    f1.plot(range(len(record_goal)), record_goal, color='red')\n",
    "    f1.set_title(\"Successful Episodes\")\n",
    "    f1.set_xlabel(\"Number of trained episodes\")\n",
    "    f1.set_ylabel(\"Sucessful Episodes\")\n",
    "\n",
    "    # Plot the No. of Failing episodes\n",
    "    f2.plot(range(len(record_fail)), record_fail, color='orange')\n",
    "    f2.set_title(\"Failing Episodes\")\n",
    "    f2.set_xlabel(\"Number of trained episodes\")\n",
    "    f2.set_ylabel(\"Failed Episodes\")\n",
    "\n",
    "    # Plot the path length\n",
    "    f3.plot(range(len(record_path_length)), record_path_length, color='blue')\n",
    "    f3.set_title(\"Successful Path Length\")\n",
    "    f3.set_xlabel(\"Number of trained episodes\")\n",
    "    f3.set_ylabel(\"Path Length\")\n",
    "\n",
    "    # Plot the episode reward\n",
    "    f4.plot(range(len(reward_list)), reward_list, color='yellow')\n",
    "    f4.set_title(\"Episode reward\")\n",
    "    f4.set_xlabel(\"Number of trained episodes\")\n",
    "    f4.set_ylabel(\"Episode reward\")\n",
    "\n",
    "    plt.figure()\n",
    "    performance_list = ['Success', 'Fail']\n",
    "    color_list = ['blue', 'red']\n",
    "    plt.bar(np.arange(len(performance_bar)), performance_bar, tick_label=performance_list, color=color_list)\n",
    "    plt.title('Bar/Success and Fail')\n",
    "    plt.ylabel('Numbers')\n",
    "\n",
    "    # Show the figures\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training with Monte Carlo Control...\")\n",
    "mc_agent = MonteCarloControl(env)\n",
    "Q_mc, mc_rewards = mc_agent.train(EPISODES)\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# save the Q-table\n",
    "mc_agent.save_q_table_text('q_table_mc_4x4.txt')\n",
    "print(\"Q-table saved to 'q_table_mc_4x4.txt'\")\n",
    "\n",
    "# load the Q-table (optional, if want to test without retraining)\n",
    "mc_agent.load_q_table_text('q_table_mc_4x4.txt')\n",
    "print(\"Q-table loaded from 'q_table_mc4x4.txt'\")\n",
    "\n",
    "# visualize the policy \n",
    "print(\"visualize policy\")\n",
    "mc_agent.visualize_policy(Q_mc, env)  \n",
    "# test the policy\n",
    "print(\"test policy\")\n",
    "mc_agent.test_policy(Q_mc, env)\n",
    "# visualize the value function\n",
    "print(\"visualize value function\")\n",
    "mc_agent.visualize_value_function(Q_mc, env)\n",
    "# render the policy\n",
    "print(\"render policy\")\n",
    "mc_agent.render_policy(Q_mc, env)\n",
    "\n",
    "# plot cumulative rewards\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.cumsum(mc_rewards), label=f\"Monte Carlo Control (Map Size: {GRID_SIZE}x{GRID_SIZE})\")\n",
    "plt.title(\"Cumulative Rewards Over Episodes\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot moving average of rewards\n",
    "window_size = 10  # Moving average window\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.convolve(mc_rewards, np.ones(window_size)/window_size, mode='valid'),\n",
    "         label=f\"Monte Carlo Control (Map Size: {GRID_SIZE}x{GRID_SIZE})\")\n",
    "plt.title(f\"Moving Average of Rewards (Window Size = {window_size})\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Collect data for plotting\n",
    "record_goal = mc_agent.record_goal\n",
    "record_fail = mc_agent.record_fail\n",
    "record_path_length = mc_agent.record_path_length\n",
    "reward_list = mc_rewards\n",
    "performance_bar = [sum(record_goal), sum(record_fail)]\n",
    "\n",
    "# Call the plot_results function\n",
    "plot_results(record_goal, record_fail, record_path_length, mc_rewards, performance_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Steps per Episode\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(record_path_length, label=\"Steps per Episode\", color=\"blue\")\n",
    "plt.title(\"Steps Taken per Episode\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Steps\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot Success Rate (Rolling Average)\n",
    "window_size = 10  # Rolling window to smooth success rate\n",
    "rolling_success = np.convolve(record_goal, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(rolling_success, label=\"Success Rate (Rolling Avg)\", color=\"green\")\n",
    "plt.title(\"Moving Average of Success Rate (Window Size = 10)\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Success Rate\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "final_success_rate = sum(mc_agent.record_goal) / len(mc_agent.record_goal) * 100\n",
    "print(f\"Final Success Rate: {final_success_rate:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ME5406_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
