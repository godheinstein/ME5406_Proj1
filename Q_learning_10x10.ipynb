{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from frozenlake_env import FrozenLakeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for reproducibility\n",
    "np.random.seed(25)\n",
    "random.seed(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "GAMMA = 0.95  # discount factor\n",
    "ALPHA = 0.1  # learning rate\n",
    "EPSILON = 0.5 # exploration rate\n",
    "EPISODES = 50000  # no. of episodes\n",
    "EPSILON_DECAY = 0.995  # epsilon decay rate\n",
    "EPSILON_MIN = 0.01  # minimum epsilon\n",
    "GRID_SIZE = 10  # grid size (4x4 or 10x10)\n",
    "HOLE_FRACTION = 0.25  # fraction of holes\n",
    "USE_DEFAULT_MAP = True  # use default map if available\n",
    "\n",
    "# create environment\n",
    "env = FrozenLakeEnv(grid_size=GRID_SIZE, hole_fraction=HOLE_FRACTION, use_default_map=USE_DEFAULT_MAP)\n",
    "\n",
    "class Q_learning: \n",
    "    def __init__(self, env, gamma=GAMMA, alpha= ALPHA, epsilon=EPSILON, epsilon_decay=EPSILON_DECAY, epsilon_min=EPSILON_MIN):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.Q = defaultdict(lambda: np.zeros(len(env.actions)))\n",
    "        self.returns = defaultdict(list)\n",
    "        self.episode_rewards = []\n",
    "        self.record_goal = []\n",
    "        self.record_fail = []\n",
    "        self.record_path_length = []\n",
    "\n",
    "\n",
    "    # epsilon greedy action selection:\n",
    "    # best action is chosen with probability 1-epsilon, default is 0.9\n",
    "    # random action is chosen with probability epsilon, default is 0.1\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(list(self.env.actions.keys()))  # Explore\n",
    "        else:\n",
    "            return max(self.env.actions.keys(), key=lambda a: self.Q[state][a])  # Exploit\n",
    "\n",
    "    def train(self, episodes):\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            episode_rewards = 0\n",
    "            done = False\n",
    "            path_length = 0\n",
    "\n",
    "            # generate episode\n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                episode_rewards += reward\n",
    "                path_length += 1\n",
    "\n",
    "                # Q-learning update rule\n",
    "                best_next_action = np.argmax(self.Q[next_state])\n",
    "                td_target = reward + self.gamma * self.Q[next_state][best_next_action]\n",
    "                td_error = td_target - self.Q[state][action]\n",
    "                self.Q[state][action] += self.alpha * td_error\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            self.episode_rewards.append(episode_rewards)\n",
    "            self.record_path_length.append(path_length)\n",
    "\n",
    "            if reward == 1:\n",
    "                self.record_goal.append(1)\n",
    "                self.record_fail.append(0)\n",
    "            else:\n",
    "                self.record_goal.append(0)\n",
    "                self.record_fail.append(1)\n",
    "            \n",
    "            # self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        return self.Q, self.episode_rewards\n",
    "    \n",
    "    # Save the Q-table to a plain text file.\n",
    "    def save_q_table_text(self, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            for state, actions in self.Q.items():\n",
    "                action_values = ' '.join(map(str, actions))\n",
    "                f.write(f\"{state} {action_values}\\n\")\n",
    "                \n",
    "    # Load the Q-table from a plain text file.\n",
    "    def load_q_table_text(self, filename):\n",
    "        self.Q = defaultdict(lambda: np.zeros(len(self.env.actions)))\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                state = int(parts[0])\n",
    "                actions = np.array([float(x) for x in parts[1:]])\n",
    "                self.Q[state] = actions   \n",
    "    \n",
    "    def visualize_policy(self, Q, env):\n",
    "        policy = np.zeros((self.env.grid_size, self.env.grid_size), dtype=str)\n",
    "        for state in Q:\n",
    "            row, col = state // self.env.grid_size, state % self.env.grid_size\n",
    "            action = np.argmax(Q[state])\n",
    "            policy[row, col] = self.env.actions[action][0]  # Use first letter of action (U, D, R, L)\n",
    "        print(\"Learned Policy:\")\n",
    "        print(policy)\n",
    "\n",
    "\n",
    "    def test_policy(self, Q, env, episodes=10):\n",
    "        total_rewards = 0\n",
    "        for episode in range(episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                action = np.argmax(Q[state])  # Greedy policy\n",
    "                next_state, reward, done = env.step(action)\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "            total_rewards += episode_reward\n",
    "            print(f\"Episode {episode + 1}: Reward = {episode_reward}\")\n",
    "        print(f\"Average Reward over {episodes} episodes: {total_rewards / episodes}\")\n",
    "\n",
    "    def visualize_value_function(self, Q, env):\n",
    "        value_function = np.zeros((env.grid_size, env.grid_size))\n",
    "        for state in Q:\n",
    "            row, col = state // env.grid_size, state % env.grid_size\n",
    "            value_function[row, col] = np.max(Q[state])\n",
    "        print(\"Value Function:\")\n",
    "        print(value_function)\n",
    "\n",
    "    def render_policy(self, Q, env):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = np.argmax(Q[state])  # Greedy policy\n",
    "            next_state, reward, done = env.step(action)\n",
    "            state = next_state\n",
    "        self.env.final()  # Show the final path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(record_goal, record_fail, record_path_length, reward_list, performance_bar):\n",
    "    \"\"\"Function to plot the performance of the robot in the figures\"\"\"\n",
    "    fig = plt.figure()\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=0.7)\n",
    "    f1 = fig.add_subplot(2, 2, 1)\n",
    "    f2 = fig.add_subplot(2, 2, 2)\n",
    "    f3 = fig.add_subplot(2, 2, 3)\n",
    "    f4 = fig.add_subplot(2, 2, 4)\n",
    "\n",
    "    # Plot the No. of Successful episodes\n",
    "    f1.plot(range(len(record_goal)), record_goal, color='red')\n",
    "    f1.set_title(\"Successful Episodes\")\n",
    "    f1.set_xlabel(\"Number of trained episodes\")\n",
    "    f1.set_ylabel(\"Sucessful Episodes\")\n",
    "\n",
    "    # Plot the No. of Failing episodes\n",
    "    f2.plot(range(len(record_fail)), record_fail, color='orange')\n",
    "    f2.set_title(\"Failing Episodes\")\n",
    "    f2.set_xlabel(\"Number of trained episodes\")\n",
    "    f2.set_ylabel(\"Failed Episodes\")\n",
    "\n",
    "    # Plot the path length\n",
    "    f3.plot(range(len(record_path_length)), record_path_length, color='blue')\n",
    "    f3.set_title(\"Successful Path Length\")\n",
    "    f3.set_xlabel(\"Number of trained episodes\")\n",
    "    f3.set_ylabel(\"Path Length\")\n",
    "\n",
    "    # Plot the episode reward\n",
    "    f4.plot(range(len(reward_list)), reward_list, color='yellow')\n",
    "    f4.set_title(\"Episode reward\")\n",
    "    f4.set_xlabel(\"Number of trained episodes\")\n",
    "    f4.set_ylabel(\"Episode reward\")\n",
    "\n",
    "    plt.figure()\n",
    "    performance_list = ['Success', 'Fail']\n",
    "    color_list = ['blue', 'red']\n",
    "    plt.bar(np.arange(len(performance_bar)), performance_bar, tick_label=performance_list, color=color_list)\n",
    "    plt.title('Bar/Success and Fail')\n",
    "    plt.ylabel('Numbers')\n",
    "\n",
    "    # Show the figures\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training with Q Learning...\")\n",
    "ql_agent = Q_learning(env)\n",
    "Q_ql, ql_rewards = ql_agent.train(EPISODES)\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# save the Q-table\n",
    "ql_agent.save_q_table_text('q_table_q_learning_10x10.txt')\n",
    "print(\"Q-table saved to 'q_table_q_learning_10x10.txt'\")\n",
    "\n",
    "# load the Q-table (optional, if want to test without retraining)\n",
    "ql_agent.load_q_table_text('q_table_q_learning_10x10.txt')\n",
    "print(\"Q-table loaded from 'q_table_q_learning_10x10.txt'\")\n",
    "\n",
    "# # visualize the policy \n",
    "ql_agent.visualize_policy(Q_ql, env)  \n",
    "# # test the policy\n",
    "ql_agent.test_policy(Q_ql, env)\n",
    "# # visualize the value function\n",
    "ql_agent.visualize_value_function(Q_ql, env)\n",
    "# # render the policy\n",
    "ql_agent.render_policy(Q_ql, env)\n",
    "\n",
    "# plot cumulative rewards\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.cumsum(ql_rewards), label=f\"Q Learning (Map Size: {GRID_SIZE}x{GRID_SIZE})\")\n",
    "plt.title(\"Cumulative Rewards Over Episodes\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot moving average of rewards\n",
    "window_size = 50  # Moving average window\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.convolve(ql_rewards, np.ones(window_size)/window_size, mode='valid'),\n",
    "         label=f\"Q Learning (Map Size: {GRID_SIZE}x{GRID_SIZE})\")\n",
    "plt.title(f\"Moving Average of Rewards (Window Size = {window_size})\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Collect data for plotting\n",
    "record_goal = ql_agent.record_goal\n",
    "record_fail = ql_agent.record_fail\n",
    "record_path_length = ql_agent.record_path_length\n",
    "reward_list = ql_rewards\n",
    "performance_bar = [sum(record_goal), sum(record_fail)]\n",
    "\n",
    "# Call the plot_results function\n",
    "plot_results(record_goal, record_fail, record_path_length, ql_rewards, performance_bar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ME5406_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
